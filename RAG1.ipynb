{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MedQA Retrieval System - Complete Implementation\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import faiss\n",
    "import streamlit as st\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# 1. Data Preparation\n",
    "\n",
    "def load_medqa_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load MedQA dataset from JSON file.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def process_textbooks(textbook_dir: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Process textbooks into sections.\"\"\"\n",
    "    sections = []\n",
    "    for filename in os.listdir(textbook_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(textbook_dir, filename), 'r') as f:\n",
    "                content = f.read()\n",
    "                # Split content into sections (this is a simple split, you might need a more sophisticated approach)\n",
    "                chapter_sections = content.split('\\n\\n')\n",
    "                for i, section in enumerate(chapter_sections):\n",
    "                    sections.append({\n",
    "                        'id': f\"{filename}_{i}\",\n",
    "                        'content': section.strip()\n",
    "                    })\n",
    "    return sections\n",
    "\n",
    "# 2. Embedding and Indexing\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "    def embed(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Create embeddings for a list of texts.\"\"\"\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "        return model_output.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "def create_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatL2:\n",
    "    \"\"\"Create a FAISS index from embeddings.\"\"\"\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 3. Hybrid Search Implementation\n",
    "\n",
    "def sparse_retrieval(query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Implement sparse retrieval using TF-IDF and cosine similarity.\"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    doc_vectors = vectorizer.fit_transform(documents)\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    return list(zip(top_indices, similarities[top_indices]))\n",
    "\n",
    "def dense_retrieval(query_embedding: np.ndarray, index: faiss.IndexFlatL2, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Implement dense retrieval using FAISS.\"\"\"\n",
    "    distances, indices = index.search(query_embedding.reshape(1, -1), top_k)\n",
    "    return list(zip(indices[0], 1 / (1 + distances[0])))  # Convert distance to similarity score\n",
    "\n",
    "def hybrid_search(query: str, dense_index: faiss.IndexFlatL2, documents: List[str], embedder: Embedder, alpha: float = 0.5, top_k: int = 5) -> List[Tuple[int, float]]:\n",
    "    \"\"\"Implement hybrid search with variable alpha.\"\"\"\n",
    "    query_embedding = embedder.embed([query])\n",
    "    dense_results = dense_retrieval(query_embedding, dense_index, top_k)\n",
    "    sparse_results = sparse_retrieval(query, documents, top_k)\n",
    "    \n",
    "    combined_results = {}\n",
    "    for idx, score in dense_results:\n",
    "        combined_results[idx] = alpha * score\n",
    "    \n",
    "    for idx, score in sparse_results:\n",
    "        if idx in combined_results:\n",
    "            combined_results[idx] += (1 - alpha) * score\n",
    "        else:\n",
    "            combined_results[idx] = (1 - alpha) * score\n",
    "    \n",
    "    return sorted(combined_results.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# 4. Question Answering Model\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "class QuestionAnsweringModel:\n",
    "    def __init__(self, model_name: str = 't5-base'):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    \n",
    "    def generate_answer(self, question: str, context: str) -> str:\n",
    "        input_text = f\"question: {question} context: {context}\"\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids, max_length=50)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 5. Streamlit UI\n",
    "\n",
    "def create_ui(qa_model: QuestionAnsweringModel, embedder: Embedder, dense_index: faiss.IndexFlatL2, documents: List[str]):\n",
    "    st.title(\"MedQA Chatbot\")\n",
    "    \n",
    "    query = st.text_input(\"Ask a medical question:\")\n",
    "    alpha = st.slider(\"Set alpha value for hybrid search:\", 0.0, 1.0, 0.5)\n",
    "    \n",
    "    if query:\n",
    "        results = hybrid_search(query, dense_index, documents, embedder, alpha)\n",
    "        context = \" \".join([documents[idx] for idx, _ in results])\n",
    "        answer = qa_model.generate_answer(query, context)\n",
    "        st.write(f\"Answer: {answer}\")\n",
    "        \n",
    "        st.subheader(\"Retrieved Contexts:\")\n",
    "        for idx, score in results:\n",
    "            st.write(f\"Score: {score:.4f}\")\n",
    "    \n",
    "    st.markdown(\"---\")\n",
    "    st.subheader(\"About\")\n",
    "    st.write(\"This chatbot uses a hybrid search method to retrieve relevant information from medical textbooks and generate answers to your questions.\")\n",
    "\n",
    "# 6. Evaluation\n",
    "\n",
    "def evaluate_system(qa_model: QuestionAnsweringModel, embedder: Embedder, dense_index: faiss.IndexFlatL2, documents: List[str], test_data: pd.DataFrame) -> Dict[str, float]:\n",
    "    correct = 0\n",
    "    total = len(test_data)\n",
    "    \n",
    "    for _, row in test_data.iterrows():\n",
    "        question = row['question']\n",
    "        correct_answer = row['answer']\n",
    "        \n",
    "        results = hybrid_search(question, dense_index, documents, embedder)\n",
    "        context = \" \".join([documents[idx] for idx, _ in results])\n",
    "        generated_answer = qa_model.generate_answer(question, context)\n",
    "        \n",
    "        if generated_answer.lower() == correct_answer.lower():\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "# Main execution\n",
    "\n",
    "def main():\n",
    "    # Load and process data\n",
    "    medqa_data = load_medqa_data('path_to_medqa_us_data.json')\n",
    "    textbook_sections = process_textbooks('path_to_textbooks_directory')\n",
    "    documents = [section['content'] for section in textbook_sections]\n",
    "    \n",
    "    # Create embeddings and index\n",
    "    embedder = Embedder()\n",
    "    doc_embeddings = embedder.embed(documents)\n",
    "    dense_index = create_faiss_index(doc_embeddings)\n",
    "    \n",
    "    # Initialize QA model\n",
    "    qa_model = QuestionAnsweringModel()\n",
    "    \n",
    "    # Create UI\n",
    "    create_ui(qa_model, embedder, dense_index, documents)\n",
    "    \n",
    "    # Evaluation\n",
    "    test_data = medqa_data[medqa_data['split'] == 'test']\n",
    "    evaluation_results = evaluate_system(qa_model, embedder, dense_index, documents, test_data)\n",
    "    st.sidebar.subheader(\"System Evaluation\")\n",
    "    st.sidebar.write(f\"Accuracy: {evaluation_results['accuracy']:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
